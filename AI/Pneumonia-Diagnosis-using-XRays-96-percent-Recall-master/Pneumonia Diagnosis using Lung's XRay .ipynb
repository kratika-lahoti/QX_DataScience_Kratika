{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pneumonia Diagnosis using Lungs' XRays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chest_xray', 'test', 'train', 'val', '__MACOSX']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import cv2                 \n",
    "import numpy as np         \n",
    "import os                  \n",
    "from random import shuffle\n",
    "from tqdm import tqdm  \n",
    "import scipy\n",
    "import skimage\n",
    "from skimage.transform import resize\n",
    "print(os.listdir(\"E:\\\\chest-xray-pneumonia\\\\chest_xray\\\\\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "99966688a2a2a8cbb5cf35aeac5e0126b0a1b7f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NORMAL', 'PNEUMONIA']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"E:\\\\chest-xray-pneumonia\\\\chest_xray\\\\train\\\\\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"E:\\\\chest-xray-pneumonia\\\\chest_xray\\\\train\\\\\"\n",
    "TEST_DIR =  \"E:\\\\chest-xray-pneumonia\\\\chest_xray\\\\test\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "fc9065d2b4d6c9efa44dc5d91bd7f2de00c1aedc"
   },
   "outputs": [],
   "source": [
    "def get_label(Dir):\n",
    "    for nextdir in os.listdir(Dir):\n",
    "        if not nextdir.startswith('.'):\n",
    "            if nextdir in ['NORMAL']:\n",
    "                label = 0\n",
    "            elif nextdir in ['PNEUMONIA']:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 2\n",
    "    return nextdir, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "f4eb2d75f5f7281d78aa3f660b04a88167480184"
   },
   "outputs": [],
   "source": [
    "def preprocessing_data(Dir):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for nextdir in os.listdir(Dir):\n",
    "        nextdir, label = get_label(Dir)\n",
    "        temp = Dir + nextdir\n",
    "        \n",
    "        for image_filename in tqdm(os.listdir(temp)):\n",
    "            path = os.path.join(temp + '/' , image_filename)\n",
    "            img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
    "            if img is not None:\n",
    "                img = skimage.transform.resize(img, (150, 150, 3))\n",
    "                img = np.asarray(img)\n",
    "                X.append(img)\n",
    "                y.append(label)\n",
    "            \n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "f2812029451339945959e2beb9ef67b0ef8acb24"
   },
   "outputs": [],
   "source": [
    "# X_train, y_train = preprocessing_data(TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "92f8a36e8392ec7d61013478d2336afcebffa94f"
   },
   "outputs": [],
   "source": [
    "def get_data(Dir):\n",
    "    X = []\n",
    "    y = []\n",
    "    for nextDir in os.listdir(Dir):\n",
    "        if not nextDir.startswith('.'):\n",
    "            if nextDir in ['NORMAL']:\n",
    "                label = 0\n",
    "            elif nextDir in ['PNEUMONIA']:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 2\n",
    "                \n",
    "            temp = Dir + nextDir\n",
    "                \n",
    "            for file in tqdm(os.listdir(temp)):\n",
    "                img = cv2.imread(temp + '/' + file)\n",
    "                if img is not None:\n",
    "                    img = skimage.transform.resize(img, (150, 150, 3))\n",
    "                    #img_file = scipy.misc.imresize(arr=img_file, size=(150, 150, 3))\n",
    "                    img = np.asarray(img)\n",
    "                    X.append(img)\n",
    "                    y.append(label)\n",
    "                    \n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "255b740623b11b5073d721030762fd615b88fe9d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1341/1341 [18:58<00:00,  2.42it/s]\n",
      "100%|██████████████████████████████████████| 3875/3875 [21:07<00:00,  4.60it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = get_data(TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "7a0c6725c477f382572b56a72b4fd7bcc61770bb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 234/234 [04:11<00:00,  1.01s/it]\n",
      "100%|████████████████████████████████████████| 390/390 [01:41<00:00,  2.87it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test , y_test = get_data(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "dcc0d8d64482e18b6751c75159327244bc90f8dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5216, 150, 150, 3) \n",
      " (624, 150, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,'\\n',X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "1d12c874fc40f26fd9b8e6c3855fc449dab1bbbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5216,) \n",
      " (624,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape,'\\n',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "93cd5729fe2e58c9ca77996fa182cfc614a40917"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train, 2)\n",
    "y_test = to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The arrays are not normalized because they have already been provided in the necessary format.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "3eec3151962cf07ddee8448cbcbb0d31fa2cfdcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5216, 2) \n",
      " (624, 2)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape,'\\n',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "c31cc3c0880ad10a71c979bdab024a15b814d334"
   },
   "outputs": [],
   "source": [
    "Pimages = os.listdir(TRAIN_DIR + \"PNEUMONIA\")\n",
    "Nimages = os.listdir(TRAIN_DIR + \"NORMAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "59c473cc8874b77f2ad6e08843fd327da942dfc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Left) - No Pneumonia Vs (Right) - Pneumonia\n",
      "-----------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Left) - No Pneumonia Vs (Right) - Pneumonia\n",
      "-----------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Left) - No Pneumonia Vs (Right) - Pneumonia\n",
      "-----------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Left) - No Pneumonia Vs (Right) - Pneumonia\n",
      "-----------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Left) - No Pneumonia Vs (Right) - Pneumonia\n",
      "-----------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plotter(i):\n",
    "    imagep1 = cv2.imread(TRAIN_DIR+\"PNEUMONIA/\"+Pimages[i])\n",
    "    imagep1 = skimage.transform.resize(imagep1, (150, 150, 3) , mode = 'reflect')\n",
    "    imagen1 = cv2.imread(TRAIN_DIR+\"NORMAL/\"+Nimages[i])\n",
    "    imagen1 = skimage.transform.resize(imagen1, (150, 150, 3))\n",
    "    pair = np.concatenate((imagen1, imagep1), axis=1)\n",
    "    print(\"(Left) - No Pneumonia Vs (Right) - Pneumonia\")\n",
    "    print(\"-----------------------------------------------------------------------------------------------------------------------------------\")\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(pair)\n",
    "    plt.show()\n",
    "for i in range(0,5):\n",
    "    plotter(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "63ff6fcfd5aa6650097a8a7d5d0858338b7c3e59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1d915cf8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "count = y_train.sum(axis = 0)\n",
    "sns.countplot(x = count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The classes are imbalanced therefore validation accuracy won't be a good metric to analyze the model performance , We will also have to take precision , recall and confusion matrix into account.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "a6003a267e5e97728a2f9285f67f4e81ab0d2b62"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\piyush.lahoti\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau , ModelCheckpoint\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_acc', factor=0.1, epsilon=0.0001, patience=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Callbacks to reduce learning rate timely after monitoring a quantity.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "8dc46c73181cbb9e837f6ab498a3220932c86733"
   },
   "outputs": [],
   "source": [
    "filepath=\"weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Making checkpoints timely to check and save the best model performance till last and also avoiding further validation accuracy drop due to overfitting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "ed6fe4cbd06dd13194092344442cc718d93f6b7d"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD , RMSprop\n",
    "from keras.layers import Conv2D , BatchNormalization\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "e72ef6b28c97a727a2c31490bd280faa86719fec"
   },
   "outputs": [],
   "source": [
    "X_train=X_train.reshape(5216,3,150,150)\n",
    "X_test=X_test.reshape(624,3,150,150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tried different model architectures , the best I could achieve was 83.75 % validation accuracy without any pre-trained CNN models. The architecture is different from the best and could give 83.01 %  . But again our main criteria is not accuracy but the precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Hyperparameters like learning rates, epochs, batch size , no. of filters , activation function have been tuned repeatedly to achieve better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "2045e6d4442a6a30297237d0e963e08bd8c3f48b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\piyush.lahoti\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\piyush.lahoti\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\piyush.lahoti\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\piyush.lahoti\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\piyush.lahoti\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\piyush.lahoti\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\piyush.lahoti\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\piyush.lahoti\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\piyush.lahoti\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 16, 150, 150)      448       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 150, 150)      2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 75, 75)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 75, 75)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 75, 75)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 37, 37)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 64, 37, 37)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 64, 37, 37)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 64, 18, 18)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 96, 18, 18)        55392     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 96, 16, 16)        83040     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 96, 8, 8)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 128, 8, 8)         110720    \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 128, 6, 6)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 128, 3, 3)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                73792     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 542,738\n",
      "Trainable params: 542,738\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def swish_activation(x):\n",
    "    return (K.sigmoid(x) * x)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', padding=\"same\", input_shape=(3,150,150)))\n",
    "model.add(Conv2D(16, (3, 3), padding=\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding=\"same\", input_shape=(3,150,150)))\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(96, (3, 3), dilation_rate=(2, 2), activation='relu', padding=\"same\"))\n",
    "model.add(Conv2D(96, (3, 3), padding=\"valid\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), dilation_rate=(2, 2), activation='relu', padding=\"same\"))\n",
    "model.add(Conv2D(128, (3, 3), padding=\"valid\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64, activation=swish_activation))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(2 , activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=RMSprop(lr=0.00005),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "872c51c19987af69d74dbd13abd3c96002e2b910"
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4b25c341c262423545b23e6ebc6f73a0eaf8ae42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5216 samples, validate on 624 samples\n",
      "Epoch 1/6\n",
      "5216/5216 [==============================] - ETA: 1:06:02 - loss: 0.6937 - acc: 0.45 - ETA: 42:49 - loss: 0.6916 - acc: 0.6016 - ETA: 35:39 - loss: 0.6900 - acc: 0.64 - ETA: 31:19 - loss: 0.6868 - acc: 0.68 - ETA: 36:54 - loss: 0.6847 - acc: 0.69 - ETA: 34:39 - loss: 0.6814 - acc: 0.70 - ETA: 33:11 - loss: 0.6769 - acc: 0.72 - ETA: 31:35 - loss: 0.6720 - acc: 0.72 - ETA: 30:54 - loss: 0.6673 - acc: 0.72 - ETA: 29:33 - loss: 0.6658 - acc: 0.72 - ETA: 28:16 - loss: 0.6590 - acc: 0.72 - ETA: 27:17 - loss: 0.6578 - acc: 0.72 - ETA: 26:44 - loss: 0.6528 - acc: 0.72 - ETA: 26:14 - loss: 0.6419 - acc: 0.73 - ETA: 25:34 - loss: 0.6435 - acc: 0.73 - ETA: 24:52 - loss: 0.6382 - acc: 0.73 - ETA: 24:50 - loss: 0.6326 - acc: 0.73 - ETA: 25:46 - loss: 0.6319 - acc: 0.73 - ETA: 25:33 - loss: 0.6317 - acc: 0.72 - ETA: 24:55 - loss: 0.6314 - acc: 0.72 - ETA: 24:21 - loss: 0.6263 - acc: 0.72 - ETA: 23:57 - loss: 0.6239 - acc: 0.72 - ETA: 23:53 - loss: 0.6195 - acc: 0.73 - ETA: 23:50 - loss: 0.6146 - acc: 0.73 - ETA: 23:44 - loss: 0.6092 - acc: 0.73 - ETA: 23:13 - loss: 0.6088 - acc: 0.73 - ETA: 22:55 - loss: 0.6045 - acc: 0.73 - ETA: 22:44 - loss: 0.6087 - acc: 0.73 - ETA: 22:31 - loss: 0.6080 - acc: 0.73 - ETA: 22:16 - loss: 0.6043 - acc: 0.73 - ETA: 24:49 - loss: 0.6004 - acc: 0.74 - ETA: 26:12 - loss: 0.5988 - acc: 0.74 - ETA: 25:59 - loss: 0.6009 - acc: 0.73 - ETA: 25:51 - loss: 0.6031 - acc: 0.73 - ETA: 25:22 - loss: 0.6004 - acc: 0.73 - ETA: 24:57 - loss: 0.6020 - acc: 0.73 - ETA: 24:40 - loss: 0.6007 - acc: 0.73 - ETA: 25:00 - loss: 0.5988 - acc: 0.74 - ETA: 24:39 - loss: 0.5981 - acc: 0.74 - ETA: 24:21 - loss: 0.5986 - acc: 0.74 - ETA: 23:58 - loss: 0.5970 - acc: 0.74 - ETA: 23:38 - loss: 0.5967 - acc: 0.74 - ETA: 23:14 - loss: 0.5952 - acc: 0.74 - ETA: 23:06 - loss: 0.5939 - acc: 0.74 - ETA: 22:44 - loss: 0.5921 - acc: 0.74 - ETA: 22:24 - loss: 0.5912 - acc: 0.74 - ETA: 22:15 - loss: 0.5922 - acc: 0.74 - ETA: 22:47 - loss: 0.5895 - acc: 0.74 - ETA: 23:19 - loss: 0.5874 - acc: 0.74 - ETA: 23:39 - loss: 0.5901 - acc: 0.74 - ETA: 24:02 - loss: 0.5889 - acc: 0.74 - ETA: 24:19 - loss: 0.5944 - acc: 0.73 - ETA: 24:10 - loss: 0.5934 - acc: 0.74 - ETA: 24:11 - loss: 0.5950 - acc: 0.73 - ETA: 24:15 - loss: 0.5960 - acc: 0.73 - ETA: 24:44 - loss: 0.5962 - acc: 0.73 - ETA: 24:28 - loss: 0.5964 - acc: 0.73 - ETA: 24:48 - loss: 0.5976 - acc: 0.73 - ETA: 24:51 - loss: 0.5971 - acc: 0.73 - ETA: 24:27 - loss: 0.5964 - acc: 0.73 - ETA: 23:59 - loss: 0.5950 - acc: 0.73 - ETA: 23:40 - loss: 0.5950 - acc: 0.73 - ETA: 23:19 - loss: 0.5931 - acc: 0.73 - ETA: 23:01 - loss: 0.5946 - acc: 0.73 - ETA: 22:52 - loss: 0.5952 - acc: 0.73 - ETA: 22:38 - loss: 0.5949 - acc: 0.73 - ETA: 22:36 - loss: 0.5960 - acc: 0.73 - ETA: 22:19 - loss: 0.5950 - acc: 0.73 - ETA: 22:00 - loss: 0.5939 - acc: 0.73 - ETA: 21:55 - loss: 0.5935 - acc: 0.73 - ETA: 21:33 - loss: 0.5943 - acc: 0.73 - ETA: 21:13 - loss: 0.5942 - acc: 0.73 - ETA: 20:59 - loss: 0.5942 - acc: 0.73 - ETA: 20:38 - loss: 0.5946 - acc: 0.73 - ETA: 20:28 - loss: 0.5929 - acc: 0.73 - ETA: 20:13 - loss: 0.5942 - acc: 0.73 - ETA: 19:55 - loss: 0.5943 - acc: 0.73 - ETA: 19:36 - loss: 0.5940 - acc: 0.73 - ETA: 19:23 - loss: 0.5947 - acc: 0.73 - ETA: 19:03 - loss: 0.5937 - acc: 0.73 - ETA: 18:45 - loss: 0.5929 - acc: 0.73 - ETA: 18:26 - loss: 0.5909 - acc: 0.73 - ETA: 18:07 - loss: 0.5909 - acc: 0.73 - ETA: 17:48 - loss: 0.5909 - acc: 0.73 - ETA: 17:28 - loss: 0.5918 - acc: 0.73 - ETA: 17:09 - loss: 0.5926 - acc: 0.73 - ETA: 16:50 - loss: 0.5922 - acc: 0.73 - ETA: 16:40 - loss: 0.5924 - acc: 0.73 - ETA: 16:23 - loss: 0.5908 - acc: 0.73 - ETA: 16:06 - loss: 0.5925 - acc: 0.73 - ETA: 15:48 - loss: 0.5934 - acc: 0.73 - ETA: 15:42 - loss: 0.5935 - acc: 0.73 - ETA: 15:32 - loss: 0.5931 - acc: 0.73 - ETA: 15:17 - loss: 0.5927 - acc: 0.73 - ETA: 15:00 - loss: 0.5934 - acc: 0.73 - ETA: 14:58 - loss: 0.5932 - acc: 0.73 - ETA: 14:52 - loss: 0.5931 - acc: 0.73 - ETA: 14:47 - loss: 0.5927 - acc: 0.73 - ETA: 14:49 - loss: 0.5919 - acc: 0.73 - ETA: 14:54 - loss: 0.5920 - acc: 0.73 - ETA: 14:48 - loss: 0.5907 - acc: 0.73 - ETA: 14:34 - loss: 0.5895 - acc: 0.73 - ETA: 14:27 - loss: 0.5900 - acc: 0.73 - ETA: 14:18 - loss: 0.5899 - acc: 0.73 - ETA: 13:59 - loss: 0.5910 - acc: 0.73 - ETA: 13:42 - loss: 0.5901 - acc: 0.73 - ETA: 13:23 - loss: 0.5888 - acc: 0.73 - ETA: 13:06 - loss: 0.5894 - acc: 0.73 - ETA: 12:49 - loss: 0.5888 - acc: 0.73 - ETA: 12:32 - loss: 0.5882 - acc: 0.73 - ETA: 12:14 - loss: 0.5877 - acc: 0.73 - ETA: 11:57 - loss: 0.5879 - acc: 0.73 - ETA: 11:40 - loss: 0.5880 - acc: 0.73 - ETA: 11:24 - loss: 0.5876 - acc: 0.73 - ETA: 11:11 - loss: 0.5869 - acc: 0.73 - ETA: 10:55 - loss: 0.5865 - acc: 0.73 - ETA: 10:44 - loss: 0.5857 - acc: 0.73 - ETA: 10:33 - loss: 0.5848 - acc: 0.74 - ETA: 10:16 - loss: 0.5856 - acc: 0.73 - ETA: 10:01 - loss: 0.5845 - acc: 0.74 - ETA: 9:46 - loss: 0.5838 - acc: 0.7408 - ETA: 9:35 - loss: 0.5851 - acc: 0.739 - ETA: 9:23 - loss: 0.5852 - acc: 0.739 - ETA: 9:14 - loss: 0.5856 - acc: 0.739 - ETA: 9:01 - loss: 0.5867 - acc: 0.737 - ETA: 8:50 - loss: 0.5869 - acc: 0.737 - ETA: 8:34 - loss: 0.5865 - acc: 0.737 - ETA: 8:18 - loss: 0.5859 - acc: 0.738 - ETA: 8:02 - loss: 0.5856 - acc: 0.738 - ETA: 7:46 - loss: 0.5848 - acc: 0.739 - ETA: 7:30 - loss: 0.5835 - acc: 0.740 - ETA: 7:19 - loss: 0.5844 - acc: 0.739 - ETA: 7:08 - loss: 0.5850 - acc: 0.739 - ETA: 6:56 - loss: 0.5855 - acc: 0.738 - ETA: 6:41 - loss: 0.5845 - acc: 0.739 - ETA: 6:26 - loss: 0.5841 - acc: 0.739 - ETA: 6:10 - loss: 0.5838 - acc: 0.739 - ETA: 5:55 - loss: 0.5828 - acc: 0.740 - ETA: 5:40 - loss: 0.5826 - acc: 0.740 - ETA: 5:25 - loss: 0.5826 - acc: 0.740 - ETA: 5:10 - loss: 0.5831 - acc: 0.739 - ETA: 4:55 - loss: 0.5831 - acc: 0.739 - ETA: 4:43 - loss: 0.5835 - acc: 0.738 - ETA: 4:31 - loss: 0.5836 - acc: 0.738 - ETA: 4:18 - loss: 0.5831 - acc: 0.738 - ETA: 4:04 - loss: 0.5834 - acc: 0.738 - ETA: 3:49 - loss: 0.5829 - acc: 0.738 - ETA: 3:34 - loss: 0.5831 - acc: 0.738 - ETA: 3:19 - loss: 0.5828 - acc: 0.739 - ETA: 3:04 - loss: 0.5827 - acc: 0.739 - ETA: 2:50 - loss: 0.5827 - acc: 0.738 - ETA: 2:36 - loss: 0.5822 - acc: 0.739 - ETA: 2:21 - loss: 0.5819 - acc: 0.739 - ETA: 2:07 - loss: 0.5812 - acc: 0.740 - ETA: 1:52 - loss: 0.5816 - acc: 0.739 - ETA: 1:38 - loss: 0.5809 - acc: 0.740 - ETA: 1:24 - loss: 0.5813 - acc: 0.739 - ETA: 1:09 - loss: 0.5812 - acc: 0.740 - ETA: 55s - loss: 0.5812 - acc: 0.739 - ETA: 41s - loss: 0.5803 - acc: 0.74 - ETA: 27s - loss: 0.5806 - acc: 0.74 - ETA: 13s - loss: 0.5804 - acc: 0.74 - 2409s 462ms/step - loss: 0.5797 - acc: 0.7407 - val_loss: 0.6944 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.62500, saving model to weights.hdf5\n",
      "Epoch 2/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5216/5216 [==============================] - ETA: 13:56 - loss: 0.6757 - acc: 0.65 - ETA: 27:55 - loss: 0.5881 - acc: 0.71 - ETA: 28:40 - loss: 0.5722 - acc: 0.73 - ETA: 31:54 - loss: 0.5839 - acc: 0.72 - ETA: 29:12 - loss: 0.5784 - acc: 0.73 - ETA: 29:11 - loss: 0.5964 - acc: 0.70 - ETA: 31:00 - loss: 0.6066 - acc: 0.69 - ETA: 30:09 - loss: 0.5997 - acc: 0.70 - ETA: 29:52 - loss: 0.6023 - acc: 0.70 - ETA: 28:12 - loss: 0.5995 - acc: 0.70 - ETA: 26:29 - loss: 0.5915 - acc: 0.71 - ETA: 25:15 - loss: 0.5969 - acc: 0.71 - ETA: 24:03 - loss: 0.5932 - acc: 0.71 - ETA: 22:58 - loss: 0.5955 - acc: 0.71 - ETA: 22:12 - loss: 0.5891 - acc: 0.71 - ETA: 22:06 - loss: 0.5951 - acc: 0.70 - ETA: 25:17 - loss: 0.5941 - acc: 0.70 - ETA: 26:27 - loss: 0.5927 - acc: 0.70 - ETA: 27:05 - loss: 0.5896 - acc: 0.71 - ETA: 26:27 - loss: 0.5872 - acc: 0.71 - ETA: 26:23 - loss: 0.5838 - acc: 0.71 - ETA: 25:36 - loss: 0.5798 - acc: 0.71 - ETA: 24:44 - loss: 0.5752 - acc: 0.72 - ETA: 24:12 - loss: 0.5732 - acc: 0.72 - ETA: 23:58 - loss: 0.5687 - acc: 0.72 - ETA: 23:57 - loss: 0.5734 - acc: 0.72 - ETA: 23:30 - loss: 0.5752 - acc: 0.72 - ETA: 23:13 - loss: 0.5738 - acc: 0.72 - ETA: 22:54 - loss: 0.5711 - acc: 0.72 - ETA: 22:20 - loss: 0.5734 - acc: 0.72 - ETA: 22:14 - loss: 0.5716 - acc: 0.72 - ETA: 21:49 - loss: 0.5700 - acc: 0.72 - ETA: 21:40 - loss: 0.5675 - acc: 0.72 - ETA: 21:32 - loss: 0.5664 - acc: 0.72 - ETA: 22:32 - loss: 0.5636 - acc: 0.73 - ETA: 22:40 - loss: 0.5641 - acc: 0.73 - ETA: 22:25 - loss: 0.5647 - acc: 0.72 - ETA: 22:10 - loss: 0.5647 - acc: 0.72 - ETA: 22:07 - loss: 0.5659 - acc: 0.72 - ETA: 21:41 - loss: 0.5684 - acc: 0.72 - ETA: 21:46 - loss: 0.5687 - acc: 0.72 - ETA: 21:35 - loss: 0.5709 - acc: 0.72 - ETA: 21:10 - loss: 0.5721 - acc: 0.71 - ETA: 20:46 - loss: 0.5713 - acc: 0.71 - ETA: 20:24 - loss: 0.5691 - acc: 0.72 - ETA: 20:03 - loss: 0.5655 - acc: 0.72 - ETA: 19:53 - loss: 0.5627 - acc: 0.72 - ETA: 19:32 - loss: 0.5603 - acc: 0.72 - ETA: 19:19 - loss: 0.5594 - acc: 0.72 - ETA: 19:00 - loss: 0.5594 - acc: 0.72 - ETA: 18:44 - loss: 0.5596 - acc: 0.72 - ETA: 18:34 - loss: 0.5596 - acc: 0.72 - ETA: 18:22 - loss: 0.5579 - acc: 0.72 - ETA: 18:16 - loss: 0.5577 - acc: 0.72 - ETA: 18:07 - loss: 0.5561 - acc: 0.72 - ETA: 17:51 - loss: 0.5541 - acc: 0.72 - ETA: 17:31 - loss: 0.5529 - acc: 0.72 - ETA: 17:13 - loss: 0.5529 - acc: 0.72 - ETA: 16:54 - loss: 0.5535 - acc: 0.72 - ETA: 16:39 - loss: 0.5523 - acc: 0.72 - ETA: 16:23 - loss: 0.5494 - acc: 0.73 - ETA: 16:18 - loss: 0.5499 - acc: 0.73 - ETA: 16:03 - loss: 0.5490 - acc: 0.73 - ETA: 15:47 - loss: 0.5484 - acc: 0.73 - ETA: 15:36 - loss: 0.5485 - acc: 0.73 - ETA: 15:21 - loss: 0.5471 - acc: 0.73 - ETA: 15:08 - loss: 0.5462 - acc: 0.73 - ETA: 14:57 - loss: 0.5478 - acc: 0.72 - ETA: 14:48 - loss: 0.5473 - acc: 0.73 - ETA: 15:02 - loss: 0.5464 - acc: 0.73 - ETA: 15:28 - loss: 0.5462 - acc: 0.73 - ETA: 15:45 - loss: 0.5460 - acc: 0.73 - ETA: 16:37 - loss: 0.5450 - acc: 0.73 - ETA: 16:33 - loss: 0.5454 - acc: 0.73 - ETA: 16:27 - loss: 0.5464 - acc: 0.72 - ETA: 16:13 - loss: 0.5460 - acc: 0.73 - ETA: 16:10 - loss: 0.5454 - acc: 0.73 - ETA: 15:54 - loss: 0.5451 - acc: 0.73 - ETA: 15:41 - loss: 0.5437 - acc: 0.73 - ETA: 15:27 - loss: 0.5421 - acc: 0.73 - ETA: 15:12 - loss: 0.5432 - acc: 0.73 - ETA: 14:57 - loss: 0.5421 - acc: 0.73 - ETA: 14:42 - loss: 0.5419 - acc: 0.73 - ETA: 14:29 - loss: 0.5406 - acc: 0.73 - ETA: 14:16 - loss: 0.5409 - acc: 0.73 - ETA: 14:02 - loss: 0.5406 - acc: 0.73 - ETA: 13:47 - loss: 0.5386 - acc: 0.73 - ETA: 13:36 - loss: 0.5360 - acc: 0.73 - ETA: 13:23 - loss: 0.5357 - acc: 0.73 - ETA: 13:11 - loss: 0.5356 - acc: 0.73 - ETA: 12:57 - loss: 0.5350 - acc: 0.73 - ETA: 12:44 - loss: 0.5346 - acc: 0.73 - ETA: 12:30 - loss: 0.5339 - acc: 0.73 - ETA: 12:17 - loss: 0.5317 - acc: 0.73 - ETA: 12:05 - loss: 0.5318 - acc: 0.73 - ETA: 11:52 - loss: 0.5307 - acc: 0.74 - ETA: 11:45 - loss: 0.5321 - acc: 0.73 - ETA: 11:38 - loss: 0.5317 - acc: 0.73 - ETA: 11:33 - loss: 0.5312 - acc: 0.74 - ETA: 11:21 - loss: 0.5298 - acc: 0.74 - ETA: 11:17 - loss: 0.5287 - acc: 0.74 - ETA: 11:12 - loss: 0.5292 - acc: 0.74 - ETA: 11:16 - loss: 0.5274 - acc: 0.74 - ETA: 11:33 - loss: 0.5259 - acc: 0.74 - ETA: 11:28 - loss: 0.5258 - acc: 0.74 - ETA: 11:25 - loss: 0.5246 - acc: 0.74 - ETA: 11:16 - loss: 0.5249 - acc: 0.74 - ETA: 11:19 - loss: 0.5241 - acc: 0.74 - ETA: 11:10 - loss: 0.5231 - acc: 0.74 - ETA: 11:06 - loss: 0.5224 - acc: 0.74 - ETA: 10:51 - loss: 0.5221 - acc: 0.74 - ETA: 10:38 - loss: 0.5210 - acc: 0.74 - ETA: 10:23 - loss: 0.5195 - acc: 0.74 - ETA: 10:09 - loss: 0.5183 - acc: 0.74 - ETA: 9:55 - loss: 0.5158 - acc: 0.7486 - ETA: 9:43 - loss: 0.5190 - acc: 0.748 - ETA: 9:29 - loss: 0.5186 - acc: 0.748 - ETA: 9:27 - loss: 0.5170 - acc: 0.750 - ETA: 9:27 - loss: 0.5169 - acc: 0.749 - ETA: 9:16 - loss: 0.5153 - acc: 0.750 - ETA: 9:03 - loss: 0.5146 - acc: 0.750 - ETA: 8:48 - loss: 0.5139 - acc: 0.750 - ETA: 8:35 - loss: 0.5124 - acc: 0.752 - ETA: 8:20 - loss: 0.5116 - acc: 0.753 - ETA: 8:06 - loss: 0.5110 - acc: 0.753 - ETA: 7:54 - loss: 0.5106 - acc: 0.753 - ETA: 7:44 - loss: 0.5096 - acc: 0.754 - ETA: 7:39 - loss: 0.5099 - acc: 0.753 - ETA: 7:30 - loss: 0.5091 - acc: 0.754 - ETA: 7:16 - loss: 0.5079 - acc: 0.755 - ETA: 7:03 - loss: 0.5074 - acc: 0.755 - ETA: 6:48 - loss: 0.5066 - acc: 0.755 - ETA: 6:44 - loss: 0.5052 - acc: 0.756 - ETA: 6:33 - loss: 0.5039 - acc: 0.757 - ETA: 6:19 - loss: 0.5037 - acc: 0.757 - ETA: 6:04 - loss: 0.5029 - acc: 0.758 - ETA: 5:50 - loss: 0.5031 - acc: 0.757 - ETA: 5:36 - loss: 0.5029 - acc: 0.757 - ETA: 5:22 - loss: 0.5013 - acc: 0.758 - ETA: 5:08 - loss: 0.5011 - acc: 0.758 - ETA: 4:54 - loss: 0.5008 - acc: 0.758 - ETA: 4:39 - loss: 0.5001 - acc: 0.759 - ETA: 4:25 - loss: 0.4989 - acc: 0.760 - ETA: 4:11 - loss: 0.4983 - acc: 0.760 - ETA: 3:57 - loss: 0.4987 - acc: 0.759 - ETA: 3:43 - loss: 0.4979 - acc: 0.760 - ETA: 3:31 - loss: 0.4971 - acc: 0.760 - ETA: 3:17 - loss: 0.4959 - acc: 0.761 - ETA: 3:04 - loss: 0.4952 - acc: 0.762 - ETA: 2:50 - loss: 0.4942 - acc: 0.762 - ETA: 2:38 - loss: 0.4937 - acc: 0.763 - ETA: 2:24 - loss: 0.4935 - acc: 0.763 - ETA: 2:11 - loss: 0.4931 - acc: 0.763 - ETA: 1:58 - loss: 0.4925 - acc: 0.763 - ETA: 1:45 - loss: 0.4922 - acc: 0.763 - ETA: 1:33 - loss: 0.4911 - acc: 0.764 - ETA: 1:20 - loss: 0.4894 - acc: 0.765 - ETA: 1:07 - loss: 0.4904 - acc: 0.765 - ETA: 53s - loss: 0.4898 - acc: 0.765 - ETA: 40s - loss: 0.4891 - acc: 0.76 - ETA: 26s - loss: 0.4883 - acc: 0.76 - ETA: 13s - loss: 0.4883 - acc: 0.76 - 2317s 444ms/step - loss: 0.4884 - acc: 0.7667 - val_loss: 0.5133 - val_acc: 0.7436\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.62500 to 0.74359, saving model to weights.hdf5\n",
      "Epoch 3/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4544/5216 [=========================>....] - ETA: 12:26 - loss: 0.4266 - acc: 0.76 - ETA: 21:37 - loss: 0.3941 - acc: 0.81 - ETA: 25:17 - loss: 0.3960 - acc: 0.81 - ETA: 30:29 - loss: 0.4325 - acc: 0.80 - ETA: 28:43 - loss: 0.4371 - acc: 0.81 - ETA: 26:38 - loss: 0.4156 - acc: 0.83 - ETA: 24:42 - loss: 0.4158 - acc: 0.82 - ETA: 22:45 - loss: 0.4544 - acc: 0.81 - ETA: 22:22 - loss: 0.4542 - acc: 0.80 - ETA: 22:49 - loss: 0.4451 - acc: 0.82 - ETA: 22:36 - loss: 0.4406 - acc: 0.82 - ETA: 21:46 - loss: 0.4294 - acc: 0.82 - ETA: 21:09 - loss: 0.4303 - acc: 0.82 - ETA: 20:17 - loss: 0.4251 - acc: 0.82 - ETA: 19:43 - loss: 0.4338 - acc: 0.82 - ETA: 19:47 - loss: 0.4323 - acc: 0.82 - ETA: 19:01 - loss: 0.4263 - acc: 0.82 - ETA: 18:29 - loss: 0.4277 - acc: 0.82 - ETA: 17:56 - loss: 0.4271 - acc: 0.82 - ETA: 17:37 - loss: 0.4237 - acc: 0.82 - ETA: 17:11 - loss: 0.4158 - acc: 0.82 - ETA: 18:20 - loss: 0.4111 - acc: 0.82 - ETA: 18:00 - loss: 0.4096 - acc: 0.82 - ETA: 17:59 - loss: 0.4047 - acc: 0.83 - ETA: 20:55 - loss: 0.3980 - acc: 0.83 - ETA: 21:45 - loss: 0.3951 - acc: 0.83 - ETA: 21:42 - loss: 0.4027 - acc: 0.82 - ETA: 22:07 - loss: 0.4000 - acc: 0.82 - ETA: 22:59 - loss: 0.3994 - acc: 0.82 - ETA: 23:45 - loss: 0.4033 - acc: 0.82 - ETA: 24:10 - loss: 0.3981 - acc: 0.82 - ETA: 23:55 - loss: 0.3924 - acc: 0.83 - ETA: 35:48 - loss: 0.3909 - acc: 0.83 - ETA: 38:07 - loss: 0.3940 - acc: 0.82 - ETA: 37:48 - loss: 0.3930 - acc: 0.83 - ETA: 37:11 - loss: 0.3899 - acc: 0.83 - ETA: 36:41 - loss: 0.3874 - acc: 0.83 - ETA: 35:58 - loss: 0.3868 - acc: 0.83 - ETA: 35:03 - loss: 0.3827 - acc: 0.83 - ETA: 34:12 - loss: 0.3850 - acc: 0.83 - ETA: 33:25 - loss: 0.3844 - acc: 0.83 - ETA: 32:39 - loss: 0.3821 - acc: 0.83 - ETA: 31:59 - loss: 0.3878 - acc: 0.83 - ETA: 31:16 - loss: 0.3868 - acc: 0.83 - ETA: 30:34 - loss: 0.3846 - acc: 0.83 - ETA: 29:55 - loss: 0.3847 - acc: 0.84 - ETA: 29:16 - loss: 0.3821 - acc: 0.83 - ETA: 28:49 - loss: 0.3809 - acc: 0.84 - ETA: 28:27 - loss: 0.3759 - acc: 0.84 - ETA: 28:01 - loss: 0.3799 - acc: 0.84 - ETA: 27:33 - loss: 0.3788 - acc: 0.84 - ETA: 26:58 - loss: 0.3797 - acc: 0.84 - ETA: 26:26 - loss: 0.3783 - acc: 0.84 - ETA: 25:55 - loss: 0.3847 - acc: 0.83 - ETA: 25:24 - loss: 0.3839 - acc: 0.84 - ETA: 25:03 - loss: 0.3862 - acc: 0.83 - ETA: 24:37 - loss: 0.3847 - acc: 0.83 - ETA: 24:09 - loss: 0.3842 - acc: 0.83 - ETA: 23:39 - loss: 0.3847 - acc: 0.83 - ETA: 23:13 - loss: 0.3829 - acc: 0.84 - ETA: 22:49 - loss: 0.3848 - acc: 0.83 - ETA: 22:22 - loss: 0.3836 - acc: 0.84 - ETA: 21:58 - loss: 0.3854 - acc: 0.84 - ETA: 21:31 - loss: 0.3850 - acc: 0.84 - ETA: 21:29 - loss: 0.3828 - acc: 0.84 - ETA: 21:34 - loss: 0.3815 - acc: 0.84 - ETA: 21:30 - loss: 0.3827 - acc: 0.84 - ETA: 21:27 - loss: 0.3843 - acc: 0.83 - ETA: 21:06 - loss: 0.3858 - acc: 0.83 - ETA: 20:43 - loss: 0.3858 - acc: 0.83 - ETA: 20:19 - loss: 0.3851 - acc: 0.83 - ETA: 19:56 - loss: 0.3844 - acc: 0.83 - ETA: 19:34 - loss: 0.3830 - acc: 0.83 - ETA: 19:11 - loss: 0.3824 - acc: 0.83 - ETA: 18:49 - loss: 0.3840 - acc: 0.83 - ETA: 18:27 - loss: 0.3849 - acc: 0.83 - ETA: 18:08 - loss: 0.3856 - acc: 0.83 - ETA: 17:50 - loss: 0.3857 - acc: 0.83 - ETA: 17:33 - loss: 0.3865 - acc: 0.83 - ETA: 17:26 - loss: 0.3871 - acc: 0.83 - ETA: 17:16 - loss: 0.3870 - acc: 0.83 - ETA: 17:13 - loss: 0.3850 - acc: 0.83 - ETA: 16:53 - loss: 0.3873 - acc: 0.83 - ETA: 16:35 - loss: 0.3874 - acc: 0.83 - ETA: 16:16 - loss: 0.3871 - acc: 0.83 - ETA: 15:58 - loss: 0.3863 - acc: 0.83 - ETA: 15:39 - loss: 0.3855 - acc: 0.83 - ETA: 15:21 - loss: 0.3866 - acc: 0.83 - ETA: 15:03 - loss: 0.3880 - acc: 0.83 - ETA: 14:47 - loss: 0.3870 - acc: 0.83 - ETA: 14:29 - loss: 0.3855 - acc: 0.83 - ETA: 14:12 - loss: 0.3844 - acc: 0.83 - ETA: 13:55 - loss: 0.3852 - acc: 0.83 - ETA: 13:40 - loss: 0.3848 - acc: 0.83 - ETA: 13:22 - loss: 0.3858 - acc: 0.83 - ETA: 13:08 - loss: 0.3866 - acc: 0.83 - ETA: 12:52 - loss: 0.3866 - acc: 0.83 - ETA: 12:36 - loss: 0.3872 - acc: 0.83 - ETA: 12:20 - loss: 0.3865 - acc: 0.83 - ETA: 12:06 - loss: 0.3859 - acc: 0.83 - ETA: 11:51 - loss: 0.3853 - acc: 0.83 - ETA: 11:36 - loss: 0.3860 - acc: 0.83 - ETA: 11:21 - loss: 0.3861 - acc: 0.82 - ETA: 11:07 - loss: 0.3873 - acc: 0.82 - ETA: 10:52 - loss: 0.3874 - acc: 0.82 - ETA: 10:42 - loss: 0.3871 - acc: 0.82 - ETA: 10:30 - loss: 0.3867 - acc: 0.82 - ETA: 10:16 - loss: 0.3888 - acc: 0.82 - ETA: 10:02 - loss: 0.3888 - acc: 0.82 - ETA: 9:49 - loss: 0.3884 - acc: 0.8284 - ETA: 9:34 - loss: 0.3875 - acc: 0.828 - ETA: 9:20 - loss: 0.3866 - acc: 0.829 - ETA: 9:07 - loss: 0.3865 - acc: 0.829 - ETA: 8:54 - loss: 0.3858 - acc: 0.829 - ETA: 8:41 - loss: 0.3843 - acc: 0.830 - ETA: 8:30 - loss: 0.3844 - acc: 0.830 - ETA: 8:17 - loss: 0.3835 - acc: 0.831 - ETA: 8:04 - loss: 0.3842 - acc: 0.830 - ETA: 7:52 - loss: 0.3830 - acc: 0.831 - ETA: 7:39 - loss: 0.3824 - acc: 0.831 - ETA: 7:27 - loss: 0.3829 - acc: 0.831 - ETA: 7:16 - loss: 0.3825 - acc: 0.831 - ETA: 7:04 - loss: 0.3818 - acc: 0.831 - ETA: 6:51 - loss: 0.3815 - acc: 0.831 - ETA: 6:40 - loss: 0.3802 - acc: 0.832 - ETA: 6:28 - loss: 0.3792 - acc: 0.832 - ETA: 6:16 - loss: 0.3782 - acc: 0.833 - ETA: 6:04 - loss: 0.3780 - acc: 0.833 - ETA: 5:53 - loss: 0.3777 - acc: 0.833 - ETA: 5:41 - loss: 0.3785 - acc: 0.832 - ETA: 5:31 - loss: 0.3778 - acc: 0.833 - ETA: 5:20 - loss: 0.3770 - acc: 0.833 - ETA: 5:09 - loss: 0.3764 - acc: 0.833 - ETA: 4:57 - loss: 0.3770 - acc: 0.833 - ETA: 4:46 - loss: 0.3776 - acc: 0.833 - ETA: 4:35 - loss: 0.3783 - acc: 0.832 - ETA: 4:24 - loss: 0.3785 - acc: 0.832 - ETA: 4:13 - loss: 0.3787 - acc: 0.832 - ETA: 4:02 - loss: 0.3783 - acc: 0.832 - ETA: 3:51 - loss: 0.3775 - acc: 0.832 - ETA: 3:40 - loss: 0.3765 - acc: 0.833 - ETA: 3:30 - loss: 0.3771 - acc: 0.8327"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data = (X_test , y_test) ,callbacks=[lr_reduce,checkpoint] ,\n",
    "          epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a5d099776e7b28863a150e980c7e6490338e1f54"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The model will try to overfit itself but rather save it prior going to the next epoch using necessary callbacks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For better performance use exponential decaying learning rate and specify steps_per_epoch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "_uuid": "15c959475e7fdc902a72d8e22bfe4276cc4b05be",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pred = model.predict(X_test)\n",
    "pred = np.argmax(pred,axis = 1) \n",
    "y_true = np.argmax(y_test,axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using mlxtend library for quick implementation of confusion matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "_uuid": "a01e456c126483c6929f4a0ff53da3dd1e4f17a0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT0AAAFACAYAAADOJ6uCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6518e82ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CM = confusion_matrix(y_true, pred)\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "fig, ax = plot_confusion_matrix(conf_mat=CM ,  figsize=(5, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, a model not good for validation accuracy might be actually good for precision or recall. So better tune according to the metric or your need.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7840670859538784"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "374 / (374 + 103)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision is of 78.40 % "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall is of 95.89 % or approx. 96 % which is quite good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.958974358974359"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "374 / (374 + 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here , recall is most significant quantity even more than accuracy and precision.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Since we are having unequal number of people in both the classes , therefore we can't take accuracy as an alone metric to calculate model efficieny ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### precision = True Positive / (True Positive + False Positive)\n",
    "\n",
    "#### recall = True Positive / (True Positive + False Negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also precision can't be taken as alone metric and has less significance than recall in this particular dataset because we have to minimize false negative and that is in the denominator and thus finally increasing 'Recall' .  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False negative has to be intuitively minimized because falsely diagnosing a patient of pneumonia as not having a pneumonia is a much larger deal than falsely diagnosing a healthy person as a pneumonia patient which is our major concern . That is why we are making this model . To reduce the mistakes done by doctors accidentally ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**-------------Data Science has always been much about context and intuition.--------------------------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
