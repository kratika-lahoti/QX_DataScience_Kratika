{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the 50-dimensional GloVe vectors\n",
    "def read_glove_vecs(file):\n",
    "    with open(file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        \n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            word = line[0]\n",
    "            words.add(word)\n",
    "            word_to_vec_map[word] = np.array(line[1:], dtype=np.float64)\n",
    "            \n",
    "    return words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D://cui2vec_pretrained.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-f4dfb59b00d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_to_vec_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_glove_vecs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D://cui2vec_pretrained.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-60f63ae44214>\u001b[0m in \u001b[0;36mread_glove_vecs\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# read in the 50-dimensional GloVe vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_glove_vecs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mword_to_vec_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D://cui2vec_pretrained.txt'"
     ]
    }
   ],
   "source": [
    "#GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space\n",
    "words, word_to_vec_map = read_glove_vecs('D://cui2vec_pretrained.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D://disease-symptom-db.csv','\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Disease</th>\n",
       "      <th>Symptom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C0162565</td>\n",
       "      <td>C0039239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C0162565</td>\n",
       "      <td>C0000737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C0162565</td>\n",
       "      <td>C0235704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C0162565</td>\n",
       "      <td>C0030554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C0162565</td>\n",
       "      <td>C0030552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C0162565</td>\n",
       "      <td>C0020538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C0162566</td>\n",
       "      <td>C0020555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C0162566</td>\n",
       "      <td>C0005758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C0004238</td>\n",
       "      <td>C0030252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C0004238</td>\n",
       "      <td>C1868917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Disease   Symptom\n",
       "0  C0162565  C0039239\n",
       "1  C0162565  C0000737\n",
       "2  C0162565  C0235704\n",
       "3  C0162565  C0030554\n",
       "4  C0162565  C0030552\n",
       "5  C0162565  C0020538\n",
       "6  C0162566  C0020555\n",
       "7  C0162566  C0005758\n",
       "8  C0004238  C0030252\n",
       "9  C0004238  C1868917"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorganises the dataframe by grouping the data by symptoms instead of by diseases\n",
    "frame = pd.DataFrame(df.groupby(['Symptom', 'Disease']).size()).drop(0, axis = 1)\n",
    "# the first entry contains only the disease and no symptom, so it is dropped\n",
    "frame = frame.iloc[1:]\n",
    "\n",
    "# set the index of the dataframe as 'Symptom'\n",
    "frame = frame.reset_index().set_index('Symptom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the counts of each symptom, ie, how many times it occurs in the data set\n",
    "counts = {}\n",
    "for i in frame.index:\n",
    "    counts[i] = counts.get(i, 0) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the symptoms by their counts in descending order and save it into a dataframe\n",
    "import operator\n",
    "sym, ct = zip(*sorted(counts.items(), key = operator.itemgetter(1), reverse = True))\n",
    "sym_count = pd.DataFrame()\n",
    "sym_count['Symptom'] = sym\n",
    "sym_count['Count'] =  ct\n",
    "#sym_count.to_csv('Symptom Counts.csv')\n",
    "\n",
    "# drop the symptoms that have fewer than 6 entries in the data set\n",
    "[frame.drop(i, inplace = True) for i in frame.index if counts[i] < 2]\n",
    "\n",
    "# extract all the diseases present in the data set and make them into a list, for use later on\n",
    "diseaseLst = []\n",
    "frame.Disease.map(lambda x: diseaseLst.append(x))\n",
    "lst = diseaseLst\n",
    "# For us to train our own word embeddings on top of the existing GloVe representation, we are going to use the skipgram model.\n",
    "# Each symptom has a disease associated with it, and we use this as the (target word, context word) pair for skipgram generation.\n",
    "# The 'skipgrams' function in Keras samples equal no. of context and non-context words for a given word from the distribution,\n",
    "# and we are going to do the same here.\n",
    "# First we'll make a list that stores the pair and its corresponding label of 1, if the disease is indeed associated with \n",
    "# the symptom, and 0 otherwise.\n",
    "couples_and_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# run through the symptoms\n",
    "for i in frame.index.unique():\n",
    "    # make a temporary list of the diseases associated with the symptom (actual context words)\n",
    "    a = list(frame.Disease.loc[i].values)\n",
    "    # loop through the context words\n",
    "    for j in a:\n",
    "        # randomly select a disease that isn't associated with the symptom, to set as a non-context word with label 0,\n",
    "        # by using the XOR operator, that finds the uncommon elements in the 2 sets\n",
    "        non_context = random.choice(list(set(lst) ^ set(a)))\n",
    "        # add labels of 1 and 0 to context and non-context words repectively\n",
    "        couples_and_labels.append((i, j, 1))\n",
    "        couples_and_labels.append((i, non_context, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the entries in the couples_and_labels list now follow the pattern of 1, 0, 1, 0 for the labels. We shuffle it up.\n",
    "b = random.sample(couples_and_labels, len(couples_and_labels))\n",
    "# Extract the symptoms, the diseases and the corresponding labels\n",
    "symptom, disease, label = zip(*b)\n",
    "\n",
    "np.save('D://symptom.npy', symptom) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform them into series' to get unique entries in each ('set()' not used as it generates a different order each time \n",
    "# and the index(number) associated with a word changes each time the program is run)\n",
    "s1 = pd.Series(list(symptom))\n",
    "s2 = pd.Series(list(disease))\n",
    "dic = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each word in the symptoms and diseases to a corresponding number that can be fed into Keras\n",
    "for i,j in enumerate(s1.append(s2).unique()):\n",
    "    dic[j] = i\n",
    "# Now all the symptoms are represented by a number in the arrays 'symptoms', and 'diseases'\n",
    "symptoms = np.array(s1.map(dic), dtype = 'int32')\n",
    "diseases = np.array(s2.map(dic), dtype = 'int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the labels too into an array\n",
    "labels = np.array(label, dtype = 'int32')\n",
    "\n",
    "dictDf = pd.DataFrame.from_dict(dic, orient='index')\n",
    "dictDf.reset_index(drop=False, inplace=True)\n",
    "dictDf.columns = ['Key', 'Values']\n",
    "dictDf.to_csv('D://Dictionary.csv', index=True, header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('D://Dictionary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Key</th>\n",
       "      <th>Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>C0026821</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>C2363736</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>C0002962</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>C0042963</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>C0011991</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>C0012833</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>C3887611</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>C0015672</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>C0003864</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>C0013595</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       Key  Values\n",
       "0           0  C0026821       0\n",
       "1           1  C2363736       1\n",
       "2           2  C0002962       2\n",
       "3           3  C0042963       3\n",
       "4           4  C0011991       4\n",
       "5           5  C0012833       5\n",
       "6           6  C3887611       6\n",
       "7           7  C0015672       7\n",
       "8           8  C0003864       8\n",
       "9           9  C0013595       9"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the vocabulary ,ie, no. of unique words in corpus\n",
    "vocab_size = len(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "547"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension of word embeddings\n",
    "vector_dim = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array of zeros of shape (vocab_size, vector_dim) to store the new embedding matrix (word vector representations)\n",
    "embedding_matrix = np.zeros((len(dic), 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_to_vec_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-c3f5954cf82a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# split each symptom/disease into a list of constituent words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_to_vec_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#only if in glove!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m             \u001b[0mlst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_to_vec_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# add the embeddings of each word in symptoms and diseases to list 'lst'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_to_vec_map' is not defined"
     ]
    }
   ],
   "source": [
    "# loop through the dictionary of words and corresponding indexes\n",
    "for word, index in dic.items():\n",
    "    # split each symptom/disease into a list of constituent words\n",
    "    for i in word.split():\n",
    "        if i in word_to_vec_map.keys(): #only if in glove!\n",
    "            lst.append(word_to_vec_map[i]) # add the embeddings of each word in symptoms and diseases to list 'lst'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an array out of the list    \n",
    "arr = np.array(lst) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum the embeddings of all words in the sentence, to get an embedding of the entire sentence\n",
    "    # if in the entire sentence, word embeddings weren't available in GloVe vectors, make that sentence into a\n",
    "    # zero array of shape (50,), just as a precaution, as we have already removed such words\n",
    "arrsum = arr.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\piyush.lahoti\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# normalize the values\n",
    "arrsum = arrsum/np.sqrt((arrsum**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the embedding to the corresponding word index\n",
    "embedding_matrix[index,:] = arrsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import necessary keras modules\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dot, Reshape, Dense, Input, Embedding\n",
    "from keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\piyush.lahoti\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\piyush.lahoti\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# START BUILDING THE KERAS MODEL FOR TRAINING\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a Keras embedding layer of shape (vocab_size, vector_dim) and set 'trainable' argument to 'True'\n",
    "embedding = Embedding(\n",
    "    input_dim = vocab_size,\n",
    "    output_dim = vector_dim,\n",
    "    input_length = 1,\n",
    "    name='embedding',\n",
    "    trainable = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\piyush.lahoti\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\piyush.lahoti\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\piyush.lahoti\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained weights(embeddings) from 'embedding_matrix' into the Keras embedding layer\n",
    "embedding.build((None,))\n",
    "embedding.set_weights([embedding_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the context and target words through the embedding layer\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vector_dim, 1))(context)\n",
    "\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'reshape_2/Reshape:0' shape=(?, 500, 1) dtype=float32>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'reshape_1/Reshape:0' shape=(?, 500, 1) dtype=float32>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the dot product of the context and target words, to find the similarity\n",
    "# (dot product is usually a measure of similarity)\n",
    "dot = Dot(axes = 1)([context, target])\n",
    "dot = Reshape((1,))(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass it through a 'sigmoid' activation neuron;\n",
    "#this is then comapared with the value in 'label' generated from the skipgram\n",
    "out = Dense(1, activation = 'sigmoid')(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\piyush.lahoti\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\piyush.lahoti\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\piyush.lahoti\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# create model instance\n",
    "model = Model(input = [input_context, input_target],  output = out)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3596/3596 [==============================] - 1s 225us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "3596/3596 [==============================] - 1s 230us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "3596/3596 [==============================] - 1s 241us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "3596/3596 [==============================] - 1s 210us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "3596/3596 [==============================] - 1s 215us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "3596/3596 [==============================] - 1s 215us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "3596/3596 [==============================] - 1s 218us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "3596/3596 [==============================] - 1s 211us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "3596/3596 [==============================] - 1s 231us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "3596/3596 [==============================] - 1s 248us/step - loss: nan - acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x105b7400>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model, default batch_size of 32\n",
    "# running for 25 epochs seems to generate good enough results, although running for more iterations\n",
    "#may improve performance further\n",
    "model.fit(x = [symptoms, diseases], y = labels, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the new weights (embeddings) after running through keras\n",
    "new_vecs = model.layers[2].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each time the model is run, it generates a different loss at the end, and consequently, different word embeddings after each run.\n",
    "# It is common to save the trained weights once they are seen to be performing well\n",
    "# I have saved the weights after 25 epochs and an end loss of 0.232, the screenshot of which I've attached, and can be loaded like this:\n",
    "\n",
    "np.save('D://similarity.npy', new_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
