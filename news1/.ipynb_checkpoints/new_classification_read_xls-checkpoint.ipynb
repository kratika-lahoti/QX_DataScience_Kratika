{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd # CSV file I/O (pd.read_csv)\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score ,confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_excel(\"C:\\\\Users\\\\Kratika\\\\Downloads\\\\News data (Completed) (1) (9).xlsx\")\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news.drop(columns = ['Unnamed: 3'])\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = news.Category.unique().tolist()\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean(Body):\n",
    "    words = [\",\", \".\", \"-\", \"/\", \"alert\", \n",
    "             \"on\",\"for\", \"from\", \"to\", \"of\", \"any\",\"in\", \"into\",\"since\", \"as\",\n",
    "             \"the\", \"a\", \"an\", \"these\",\"there\",\n",
    "             \"new\", \"or\", \"and\", \n",
    "             \"me\", \"i\", \"he\", \"his\",\"she\", \"her\" ,\"him\", \"us\",\"we\", \"our\", \"they\", \"their\",\"you\", \"your\",\"it\",\n",
    "             \"i'm\", \"he's\", \"we'r\", \"our's\", \"your's\",\"it's\",\"its\",\n",
    "             \"is\" ,\"am\", \"are\", \"have\", \"has\", \"does\",\"were\",\"was\",\"will\",\"be\" ,\"been\",\"shall\" ,\"had\",\n",
    "             \"when\", \"where\", \"what\",\"companies\",\"company\"]\n",
    "\n",
    "    clean_cat = Body.lower()\n",
    "    for word in words:\n",
    "        clean_cat = re.sub(r\"[\\s.,]\"+word+\"[\\s.,]\", ' ', clean_cat)\n",
    "    return clean_cat.strip()\n",
    "    \n",
    "\n",
    "news['clean_Body'] = news['Body'].apply(clean) \n",
    "\n",
    "news.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dic= {\"growth\": ['announces', 'arrangement', 'average sales', 'contract',\n",
    "       'customer', 'expands into a new country',\n",
    "       'expands into a new state', 'expansion', 'focus', 'focusses',\n",
    "       'growth', 'initiative', 'initiatives', 'investment',\n",
    "       'joint venture', 'license', 'moves into a new country',\n",
    "       'moves into a new state', 'new geography', 'new office', 'partner',\n",
    "       'partnership', 'patent', 'product', 'production', 'products',\n",
    "       'registration', 'regulatory approval', 'related', 'sales',\n",
    "       'short-term business', 'strategy', 'user growth', 'user volume',\n",
    "       'won'],\n",
    "        \"financial\":['acquire', 'acquired', 'acquires', 'acquisition', 'annual report',\n",
    "       'buyback', 'buys back', 'credit rating agency', 'debt',\n",
    "       'dividends', 'earnings', 'equity', 'financing', 'funding',\n",
    "       'invested', 'invests', 'ipo', 'legal paperwork', 'loan', 'merger',\n",
    "       'ownership change', 'payback', 'payback debt', 'pays back',\n",
    "       'performance', 'public offering', 'raise money',\n",
    "       'received private funding', 'regulatory', 'repay',\n",
    "       'research house', 'restructuring', 'sec', 'shareholder', 'shares',\n",
    "       'stake decrease', 'stake increase', 'stake sale', 'stock',\n",
    "       'undergo financial restructuring']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dic={'growth':['announces', 'arrangement', 'average sales', 'contract',\n",
    "       'customer', 'expands into a new country',\n",
    "       'expands into a new state', 'expansion', 'focus', 'focusses',\n",
    "       'growth', 'initiative', 'initiatives', 'investment',\n",
    "       'joint venture', 'license', 'moves into a new country',\n",
    "       'moves into a new state', 'new geography', 'new office', 'partner',\n",
    "       'partnership', 'patent', 'product', 'production', 'products',\n",
    "       'registration', 'regulatory approval', 'related', 'sales',\n",
    "       'short-term business', 'strategy', 'user growth', 'user volume',\n",
    "       'won']}\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dic.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_Body = news.clean_Body.unique().tolist()\n",
    "clean_Body.sort()\n",
    "clean_Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.set_proxy('https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml', ('USERNAME', 'PASSWORD'))\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "  \n",
    "example_sent = \"clean_Body\"\n",
    "  \n",
    "stop_words = set(stopwords.words('clean_Body')) \n",
    "  \n",
    "word_tokens = word_tokenize(example_sent) \n",
    "  \n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "filtered_sentence = [] \n",
    "  \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "  \n",
    "print(word_tokens) \n",
    "print(filtered_sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
